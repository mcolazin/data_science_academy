{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3366ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aaee54f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "01461ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "col_names = [\"class\", \"Alcohol\", \"Malic\" \"acid\", \"Ash\", \"Alcalinity of ash\", \"Magnesium\",\n",
    "             \"Total phenols\", \"Flavanoids\", \"Nonflavanoid phenols\", \"Proanthocyanins\",\n",
    "             \"Color intensity\", \"Hue\", \"OD280/OD315 of diluted wines\", \"Proline\" ]\n",
    "dados = pd.read_csv(\"wine.data\", names=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "da64163c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malicacid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>3</td>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>3</td>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>3</td>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>3</td>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>3</td>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     class  Alcohol  Malicacid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0        1    14.23       1.71  2.43               15.6        127   \n",
       "1        1    13.20       1.78  2.14               11.2        100   \n",
       "2        1    13.16       2.36  2.67               18.6        101   \n",
       "3        1    14.37       1.95  2.50               16.8        113   \n",
       "4        1    13.24       2.59  2.87               21.0        118   \n",
       "..     ...      ...        ...   ...                ...        ...   \n",
       "173      3    13.71       5.65  2.45               20.5         95   \n",
       "174      3    13.40       3.91  2.48               23.0        102   \n",
       "175      3    13.27       4.28  2.26               20.0        120   \n",
       "176      3    13.17       2.59  2.37               20.0        120   \n",
       "177      3    14.13       4.10  2.74               24.5         96   \n",
       "\n",
       "     Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0             2.80        3.06                  0.28             2.29   \n",
       "1             2.65        2.76                  0.26             1.28   \n",
       "2             2.80        3.24                  0.30             2.81   \n",
       "3             3.85        3.49                  0.24             2.18   \n",
       "4             2.80        2.69                  0.39             1.82   \n",
       "..             ...         ...                   ...              ...   \n",
       "173           1.68        0.61                  0.52             1.06   \n",
       "174           1.80        0.75                  0.43             1.41   \n",
       "175           1.59        0.69                  0.43             1.35   \n",
       "176           1.65        0.68                  0.53             1.46   \n",
       "177           2.05        0.76                  0.56             1.35   \n",
       "\n",
       "     Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0               5.64  1.04                          3.92     1065  \n",
       "1               4.38  1.05                          3.40     1050  \n",
       "2               5.68  1.03                          3.17     1185  \n",
       "3               7.80  0.86                          3.45     1480  \n",
       "4               4.32  1.04                          2.93      735  \n",
       "..               ...   ...                           ...      ...  \n",
       "173             7.70  0.64                          1.74      740  \n",
       "174             7.30  0.70                          1.56      750  \n",
       "175            10.20  0.59                          1.56      835  \n",
       "176             9.30  0.60                          1.62      840  \n",
       "177             9.20  0.61                          1.60      560  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0294a2f",
   "metadata": {},
   "source": [
    "#### Neste exercÃ­cio nosso objetivo serÃ¡ a classificaÃ§Ã£o de dados no keras. A primeira variÃ¡vel, identificada como classe, \"class\" serÃ¡ nosso label de classificaÃ§Ã£o com 3 nÃ­veis (1,2,3). Chamaremos de y.\n",
    "\n",
    "#### PorÃ©m ao importar as classes para y, teremos um array. Assim, este array deverÃ¡ ser convertido para matriz, via utils, para ser processado no Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c2c8626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = dados[\"class\"].values\n",
    "y\n",
    "### o array y deverÃ¡ ser convertido para matriz para ser usado no keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7616f378",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.]], dtype=float32),\n",
       " (178, 3))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converter o label (array ) para matriz ( one hot )\n",
    "y_train = keras.utils.to_categorical(y)\n",
    "y_train\n",
    "# sÃ£o 3 classes, e a primeiro coluna, nÃ£o representa nenhuma classe, vamos eliminar\n",
    "y_train = y_train[:, 1:4]\n",
    "y_train, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "edbbc20b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[14.23,  1.71,  2.43, ...,  5.64,  1.04,  3.92],\n",
       "        [13.2 ,  1.78,  2.14, ...,  4.38,  1.05,  3.4 ],\n",
       "        [13.16,  2.36,  2.67, ...,  5.68,  1.03,  3.17],\n",
       "        ...,\n",
       "        [13.27,  4.28,  2.26, ..., 10.2 ,  0.59,  1.56],\n",
       "        [13.17,  2.59,  2.37, ...,  9.3 ,  0.6 ,  1.62],\n",
       "        [14.13,  4.1 ,  2.74, ...,  9.2 ,  0.61,  1.6 ]]),\n",
       " (178, 12))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remover a variavel dependente do dataset de entrada\n",
    "dados_in = dados.iloc[:,1:13]\n",
    "dados_in = dados_in.values\n",
    "dados_in, dados_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c55a6887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "07657229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequencia de pilhas lineares de rede neural\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Importar camadas especÃ­ficas que serÃ£o usadas nas redes neurais\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "\n",
    "# Optimizador - Gradient Descent\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# utilitarios de transformaÃ§Ã£o dos dados\n",
    "from tensorflow.keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8492c6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Construindo a rede neural \n",
    "model = Sequential() ## Instanciando a classe Sequential() para a construÃ§Ã£o da NN\n",
    "# input_dim = quantidade de variÃ¡veis independentes\n",
    "# kernel_initializer = iniciaÃ§Ã£o dos parametros Betas e bias = normal shuffle\n",
    "model.add(Dense(40, input_dim = 12, kernel_initializer = \"normal\", activation = \"relu\")) # Dense = Uma rede neural totalmente conectada\n",
    "model.add(Dense(10, kernel_initializer = \"normal\", activation = \"sigmoid\")) # a camada oculta tem fuÃ§Ã£o de ativaÃ§Ã£o sigmoid\n",
    "model.add(Dropout(0.10)) # camada para evitar o overFit.\n",
    "model.add(Dense(5, kernel_initializer = \"normal\", activation=\"sigmoid\"))\n",
    "# 3 layers de neuronios (3 classes) - softmax para retornar a probabilidade de cada classe\n",
    "model.add(Dense(3, kernel_initializer = \"normal\", activation=\"softmax\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d4543ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OtimizaÃ§Ã£o com Stochastic Grandient Descent\n",
    "sgd = keras.optimizers.Adam(lr = 0.01, beta_1=0.9, beta_2=0.99, epsilon=0.00000001, decay = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2ff3acd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CompilaÃ§Ã£o do modelo e treino\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer = sgd, metrics=[\"acc\"]) # use categorical_cross para classificaÃ§Ã£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9cb1897c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 0s 0s/step - loss: 0.5694 - acc: 0.6573\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 508us/step - loss: 0.5729 - acc: 0.6629\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 0s/step - loss: 0.5754 - acc: 0.6517\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5578 - acc: 0.6573\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 894us/step - loss: 0.5706 - acc: 0.6517\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5569 - acc: 0.6573\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 836us/step - loss: 0.5506 - acc: 0.6629\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 520us/step - loss: 0.5514 - acc: 0.6573\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5492 - acc: 0.6573\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5396 - acc: 0.6629\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 948us/step - loss: 0.5394 - acc: 0.6629\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5446 - acc: 0.6517\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 332us/step - loss: 0.7023 - acc: 0.6180\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.6871 - acc: 0.6011\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 667us/step - loss: 0.5660 - acc: 0.6517\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 667us/step - loss: 0.5442 - acc: 0.6629\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5440 - acc: 0.6573\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 667us/step - loss: 0.5528 - acc: 0.6629\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.5558 - acc: 0.6461\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5362 - acc: 0.6573\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5418 - acc: 0.6629\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 752us/step - loss: 0.5386 - acc: 0.6629\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5325 - acc: 0.6573\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5336 - acc: 0.6629\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 667us/step - loss: 0.5400 - acc: 0.6517\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5728 - acc: 0.6517\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 667us/step - loss: 0.5515 - acc: 0.6461\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.5371 - acc: 0.6573\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5309 - acc: 0.6685\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5191 - acc: 0.6629\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 836us/step - loss: 0.5416 - acc: 0.6573\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 919us/step - loss: 0.5376 - acc: 0.6573\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 667us/step - loss: 0.5170 - acc: 0.6629\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5204 - acc: 0.6629\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 890us/step - loss: 0.5217 - acc: 0.6629\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5179 - acc: 0.6629\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 667us/step - loss: 0.5157 - acc: 0.6685\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 667us/step - loss: 0.5196 - acc: 0.6573\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5220 - acc: 0.6629\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.5353 - acc: 0.6517\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.5245 - acc: 0.6573\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5135 - acc: 0.6685\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5310 - acc: 0.6573\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 667us/step - loss: 0.5298 - acc: 0.6573\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5123 - acc: 0.6629\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 851us/step - loss: 0.5077 - acc: 0.6629\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 518us/step - loss: 0.5143 - acc: 0.6629\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.5410 - acc: 0.6517\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 971us/step - loss: 0.5315 - acc: 0.6573\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.5341 - acc: 0.6404\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5131 - acc: 0.6629\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5096 - acc: 0.6629\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5172 - acc: 0.6573\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 835us/step - loss: 0.5050 - acc: 0.6573\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5213 - acc: 0.6573\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5013 - acc: 0.6629\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4975 - acc: 0.6629\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5078 - acc: 0.6573\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5034 - acc: 0.6461\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 667us/step - loss: 0.4920 - acc: 0.7079\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4751 - acc: 0.6629\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4676 - acc: 0.7472\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 388us/step - loss: 0.4647 - acc: 0.6966\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4812 - acc: 0.7360\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4747 - acc: 0.6910\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 836us/step - loss: 0.4531 - acc: 0.7528\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 899us/step - loss: 0.4558 - acc: 0.7360\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4429 - acc: 0.7921\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 847us/step - loss: 0.4756 - acc: 0.7865\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5207 - acc: 0.7528\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 836us/step - loss: 0.4053 - acc: 0.8146\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 800us/step - loss: 0.4423 - acc: 0.7921\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.4360 - acc: 0.7978\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4315 - acc: 0.8034\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.4171 - acc: 0.8034\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4342 - acc: 0.7809\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 919us/step - loss: 0.3669 - acc: 0.8371\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 745us/step - loss: 0.3819 - acc: 0.8258\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3707 - acc: 0.8427\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3420 - acc: 0.8539\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 667us/step - loss: 0.3201 - acc: 0.9101\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3120 - acc: 0.8989\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 667us/step - loss: 0.3795 - acc: 0.8539\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3404 - acc: 0.8539\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 667us/step - loss: 0.3280 - acc: 0.8933\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 836us/step - loss: 0.3025 - acc: 0.9101\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 835us/step - loss: 0.2673 - acc: 0.9213\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 836us/step - loss: 0.2468 - acc: 0.9382\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 836us/step - loss: 0.2826 - acc: 0.8933\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 752us/step - loss: 0.2726 - acc: 0.9101\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 882us/step - loss: 0.3028 - acc: 0.8764\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2975 - acc: 0.8820\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 744us/step - loss: 0.2692 - acc: 0.9157\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 667us/step - loss: 0.2330 - acc: 0.9270\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2153 - acc: 0.9326\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.2095 - acc: 0.9382\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1996 - acc: 0.9494\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.1928 - acc: 0.9551\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2227 - acc: 0.9270\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.2058 - acc: 0.9494\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18049992620944977, 0.9438202381134033]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x= dados_in, y=y_train, epochs=100, verbose=1)\n",
    "model.evaluate(x = dados_in, y = y_train, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86c9825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8610ae96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0815c1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf1f273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c26a991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63124e14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
